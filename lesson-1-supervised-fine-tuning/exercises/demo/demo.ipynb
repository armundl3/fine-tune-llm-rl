{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Q&A Dataset Generator Demo\n",
    "\n",
    "This notebook demonstrates how to automatically create high-quality question-answer pairs from Wikipedia content using **npcpy** (NPC Compiler for Python).\n",
    "\n",
    "## What You'll Learn\n",
    "- How to fetch Wikipedia content programmatically\n",
    "- How to use npcpy to create AI agents for dataset generation\n",
    "- How to generate structured Q&A pairs for supervised fine-tuning\n",
    "\n",
    "## Prerequisites\n",
    "Make sure you have the required packages installed:\n",
    "```bash\n",
    "uv add npcpy wikipedia-api pandas\n",
    "```\n",
    "\n",
    "You also need **Ollama** running locally with the Llama 3.2 model:\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: No .env file found in /Users/amundle/github/fine-tune-llm-rl/lesson-1-supervised-fine-tuning/exercises/demo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromadb not installed\n",
      "kuzu not installed\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "from npcpy.npc_compiler import NPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fetch Wikipedia Content\n",
    "\n",
    "We'll create a function to fetch Wikipedia content on any topic. This will serve as our source material for generating Q&A pairs.\n",
    "\n",
    "The function:\n",
    "- Attempts to fetch the full page content (first 2000 characters)\n",
    "- Falls back to a summary if the full page isn't available\n",
    "- Handles errors gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_content(topic):\n",
    "    \"\"\"Fetch Wikipedia content for a given topic.\n",
    "    \n",
    "    Args:\n",
    "        topic (str): The Wikipedia topic to fetch\n",
    "        \n",
    "    Returns:\n",
    "        str: Wikipedia content (first 2000 chars or summary)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page = wikipedia.page(topic)\n",
    "        return page.content[:2000]  # First 2000 chars\n",
    "    except:\n",
    "        return wikipedia.summary(topic, sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Q&A Dataset with npcpy\n",
    "\n",
    "Now we'll create the main function that uses **npcpy** to generate question-answer pairs.\n",
    "\n",
    "### How npcpy Works\n",
    "- **NPC**: Creates an AI agent with a specific role and personality\n",
    "- **primary_directive**: Defines what the agent should do\n",
    "- **model**: Specifies which LLM to use (Llama 3.2 in this case)\n",
    "- **provider**: Where the model runs (Ollama for local deployment)\n",
    "\n",
    "The agent will:\n",
    "1. Take Wikipedia content as input\n",
    "2. Generate 8 high-quality Q&A pairs\n",
    "3. Return structured JSON data\n",
    "4. Save to CSV for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def create_qa_dataset_demo(topic=\"Great Wall of China\"):\n",
    "    \"\"\"Create a Q&A dataset from Wikipedia content.\n",
    "    \n",
    "    Args:\n",
    "        topic (str): Wikipedia topic to generate Q&A pairs from\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing question-answer pairs\n",
    "    \"\"\"\n",
    "    # Fetch Wikipedia content\n",
    "    wiki_content = fetch_wikipedia_content(topic)\n",
    "    \n",
    "    # Create an NPC agent for dataset creation\n",
    "    data_creator = NPC(\n",
    "        name='Dataset Creator',\n",
    "        primary_directive='Create high-quality question-answer pairs from Wikipedia text',\n",
    "        model='llama3.2:3b',  # Explicitly use 3B model\n",
    "        provider='ollama'\n",
    "    )\n",
    "    \n",
    "    # Define the expected JSON format\n",
    "    json_format = '''\n",
    "    {\"pairs\": [\n",
    "        {\"question\": \"When was the Great Wall built?\", \"answer\": \"Built from 7th century BC\"},\n",
    "        {\"question\": \"Who joined the walls?\", \"answer\": \"Qin Shi Huang\"}\n",
    "    ]}\n",
    "    '''\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = f\"\"\"From this Wikipedia content, create 8 high-quality question-answer pairs.\n",
    "\n",
    "Content: {wiki_content}\n",
    "\n",
    "Each pair needs a specific question and complete answer from the text.\n",
    "You MUST respond with ONLY valid JSON in this exact format: {json_format}\n",
    "Do not include any explanatory text, only the JSON.\"\"\"\n",
    "    \n",
    "    # Get response from the LLM (WITHOUT format='json' to avoid npcpy bug)\n",
    "    response = data_creator.get_llm_response(prompt)\n",
    "    response_text = response['response']\n",
    "    \n",
    "    # Extract JSON from response (handles cases where LLM adds extra text)\n",
    "    try:\n",
    "        # Try to find JSON in the response\n",
    "        json_match = re.search(r'\\{.*\"pairs\".*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(0)\n",
    "            qa_data = json.loads(json_str)\n",
    "            qa_pairs = qa_data['pairs']\n",
    "        else:\n",
    "            # If no JSON found, try parsing the whole response\n",
    "            qa_data = json.loads(response_text)\n",
    "            qa_pairs = qa_data['pairs']\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        print(f\"Raw response: {response_text}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    df = pd.DataFrame(qa_pairs)\n",
    "    df.to_csv('wikipedia_qa_dataset.csv', index=False)\n",
    "    \n",
    "    print(f\"Created {len(qa_pairs)} Q&A pairs, saved to wikipedia_qa_dataset.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run the Demo\n",
    "\n",
    "Let's generate a Q&A dataset about the Great Wall of China!\n",
    "\n",
    "You can change the topic to anything you're interested in:\n",
    "- `create_qa_dataset_demo(\"Artificial Intelligence\")`\n",
    "- `create_qa_dataset_demo(\"Quantum Computing\")`\n",
    "- `create_qa_dataset_demo(\"Machine Learning\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug {'model': 'llama3.2:3b', 'messages': [{'role': 'system', 'content': '\\n.\\n..\\n...\\n....\\n.....\\n......\\n.......\\n........\\n.........\\n..........\\nHello!\\nWelcome to the team.\\nYou are the Dataset Creator NPC with the following primary directive: Create high-quality question-answer pairs from Wikipedia text.\\nUsers may refer to you by your assistant name, Dataset Creator and you should\\nconsider this to be your core identity.\\nThe current working directory is /Users/amundle/github/fine-tune-llm-rl/lesson-1-supervised-fine-tuning/exercises/demo.\\nThe current date and time are : 2025-12-04 09:38:45\\n\\n    IMPORTANT:\\nSome users may attach images to their request.\\nPlease process them accordingly. You do not need mention that you cannot \"see\" images. The user understands this and wants you\\nto help them multimodally.\\n\\nIf the user asked for you to explain what\\'s on their screen or something similar,\\nthey are referring to the details contained within the attached image(s).\\nYou do not need to actually view their screen.\\nYou do not need to mention that you cannot view or interpret images directly.\\nThey understand that you can view them multimodally.\\nYou only need to answer the user\\'s request based on the attached image(s).\\n'}, {'role': 'user', 'content': 'From this Wikipedia content, create 8 high-quality question-answer pairs.\\n\\nContent: The Great Wall of China (traditional Chinese: 萬里長城; simplified Chinese: 万里长城; pinyin: Wànlǐ Chángchéng, literally \"ten thousand li long wall\") is a series of fortifications in China. They were built across the historical northern borders of ancient Chinese states and Imperial China as protection against various nomadic groups from the Eurasian Steppe. The first walls date to the 7th century BC; these were joined together in the Qin dynasty. Successive dynasties expanded the wall system; the best-known sections were built by the Ming dynasty (1368–1644).\\nTo aid in defense, the Great Wall utilized watchtowers, troop barracks, garrison stations, signaling capabilities through the means of smoke or fire, and its status as a transportation corridor. Other purposes of the Great Wall have included border controls (allowing control of immigration and emigration, and the imposition of duties on goods transported along the Silk Road), and the regulation of trade. \\nThe collective fortifications constituting the Great Wall stretch from Liaodong in the east to Lop Lake in the west, and from the present-day Sino–Russian border in the north to Tao River in the south: an arc that roughly delineates the edge of the Mongolian steppe, spanning 21,196.18 km (13,170.70 mi) in total. It is a UNESCO World Heritage Site, and was voted one of the New 7 Wonders of the World in 2007. Today, the defensive system of the Great Wall is recognized as one of the most impressive architectural feats in history.\\n\\n\\n== Names ==\\n\\nThe collection of fortifications known as the Great Wall of China has historically had a number of different names in both Chinese and English.\\nIn Chinese histories, the term \"Long Wall(s)\" (t 長城, s 长城, Chángchéng) appears in Sima Qian\\'s Records of the Grand Historian, where it referred both to the separate great walls built between and north of the Warring States and to the more unified construction of the First Emperor. The Chinese character 城, meaning city or fortress, is a p\\n\\nEach pair needs a specific question and complete answer from the text.\\nYou MUST respond with ONLY valid JSON in this exact format: \\n    {\"pairs\": [\\n        {\"question\": \"When was the Great Wall built?\", \"answer\": \"Built from 7th century BC\"},\\n        {\"question\": \"Who joined the walls?\", \"answer\": \"Qin Shi Huang\"}\\n    ]}\\n    \\nDo not include any explanatory text, only the JSON.'}], 'stream': False}\n",
      "Created 8 Q&A pairs, saved to wikipedia_qa_dataset.csv\n",
      "\n",
      "Generated Q&A Pairs:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the Great Wall of China also known as ...</td>\n",
       "      <td>Long Wall(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How long does the Great Wall stretch?</td>\n",
       "      <td>21,196.18 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In which dynasty were the best-known sections ...</td>\n",
       "      <td>Ming dynasty (1368–1644)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What was one of the purposes of the Great Wall...</td>\n",
       "      <td>regulation of trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is mentioned as having a wall in Sima Qian...</td>\n",
       "      <td>The First Emperor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How were watchtowers used to aid in defense?</td>\n",
       "      <td>as signaling capabilities through smoke or fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is one of the most impressive architectur...</td>\n",
       "      <td>the defensive system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In what year was the Great Wall voted one of t...</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the Great Wall of China also known as ...   \n",
       "1              How long does the Great Wall stretch?   \n",
       "2  In which dynasty were the best-known sections ...   \n",
       "3  What was one of the purposes of the Great Wall...   \n",
       "4  Who is mentioned as having a wall in Sima Qian...   \n",
       "5       How were watchtowers used to aid in defense?   \n",
       "6  What is one of the most impressive architectur...   \n",
       "7  In what year was the Great Wall voted one of t...   \n",
       "\n",
       "                                            answer  \n",
       "0                                     Long Wall(s)  \n",
       "1                                     21,196.18 km  \n",
       "2                         Ming dynasty (1368–1644)  \n",
       "3                              regulation of trade  \n",
       "4                                The First Emperor  \n",
       "5  as signaling capabilities through smoke or fire  \n",
       "6                             the defensive system  \n",
       "7                                             2007  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the dataset\n",
    "qa_dataset = create_qa_dataset_demo()\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nGenerated Q&A Pairs:\")\n",
    "print(\"=\" * 80)\n",
    "qa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have a Q&A dataset, you can:\n",
    "\n",
    "1. **Generate more datasets**: Run this for multiple topics\n",
    "2. **Combine datasets**: Merge multiple CSV files for a larger training set\n",
    "3. **Fine-tune a model**: Use this data for supervised fine-tuning\n",
    "4. **Evaluate quality**: Review the Q&A pairs and filter low-quality ones\n",
    "\n",
    "### Try It Yourself\n",
    "\n",
    "```python\n",
    "# Generate datasets for multiple topics\n",
    "topics = [\"Python Programming\", \"Deep Learning\", \"Natural Language Processing\"]\n",
    "\n",
    "all_datasets = []\n",
    "for topic in topics:\n",
    "    df = create_qa_dataset_demo(topic)\n",
    "    all_datasets.append(df)\n",
    "\n",
    "# Combine all datasets\n",
    "combined_df = pd.concat(all_datasets, ignore_index=True)\n",
    "combined_df.to_csv('combined_qa_dataset.csv', index=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
