{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Q&A Dataset Generator Demo\n",
    "\n",
    "This notebook demonstrates how to automatically create high-quality question-answer pairs from Wikipedia content using **npcpy** (NPC Compiler for Python).\n",
    "\n",
    "## What You'll Learn\n",
    "- How to fetch Wikipedia content programmatically\n",
    "- How to use npcpy to create AI agents for dataset generation\n",
    "- How to generate structured Q&A pairs for supervised fine-tuning\n",
    "\n",
    "## Prerequisites\n",
    "Make sure you have the required packages installed:\n",
    "```bash\n",
    "uv add npcpy wikipedia-api pandas\n",
    "```\n",
    "\n",
    "You also need **Ollama** running locally with the Llama 3.2 model:\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "from npcpy.npc_compiler import NPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fetch Wikipedia Content\n",
    "\n",
    "We'll create a function to fetch Wikipedia content on any topic. This will serve as our source material for generating Q&A pairs.\n",
    "\n",
    "The function:\n",
    "- Attempts to fetch the full page content (first 2000 characters)\n",
    "- Falls back to a summary if the full page isn't available\n",
    "- Handles errors gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_content(topic):\n",
    "    \"\"\"Fetch Wikipedia content for a given topic.\n",
    "    \n",
    "    Args:\n",
    "        topic (str): The Wikipedia topic to fetch\n",
    "        \n",
    "    Returns:\n",
    "        str: Wikipedia content (first 2000 chars or summary)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page = wikipedia.page(topic)\n",
    "        return page.content[:2000]  # First 2000 chars\n",
    "    except:\n",
    "        return wikipedia.summary(topic, sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Q&A Dataset with npcpy\n",
    "\n",
    "Now we'll create the main function that uses **npcpy** to generate question-answer pairs.\n",
    "\n",
    "### How npcpy Works\n",
    "- **NPC**: Creates an AI agent with a specific role and personality\n",
    "- **primary_directive**: Defines what the agent should do\n",
    "- **model**: Specifies which LLM to use (Llama 3.2 in this case)\n",
    "- **provider**: Where the model runs (Ollama for local deployment)\n",
    "\n",
    "The agent will:\n",
    "1. Take Wikipedia content as input\n",
    "2. Generate 8 high-quality Q&A pairs\n",
    "3. Return structured JSON data\n",
    "4. Save to CSV for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def create_qa_dataset_demo(topic=\"Great Wall of China\"):\n",
    "    \"\"\"Create a Q&A dataset from Wikipedia content.\n",
    "    \n",
    "    Args:\n",
    "        topic (str): Wikipedia topic to generate Q&A pairs from\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing question-answer pairs\n",
    "    \"\"\"\n",
    "    # Fetch Wikipedia content\n",
    "    wiki_content = fetch_wikipedia_content(topic)\n",
    "    \n",
    "    # Create an NPC agent for dataset creation\n",
    "    data_creator = NPC(\n",
    "        name='Dataset Creator',\n",
    "        primary_directive='Create high-quality question-answer pairs from Wikipedia text',\n",
    "        model='llama3.2:3b',  # Explicitly use 3B model\n",
    "        provider='ollama'\n",
    "    )\n",
    "    \n",
    "    # Define the expected JSON format\n",
    "    json_format = '''\n",
    "    {\"pairs\": [\n",
    "        {\"question\": \"When was the Great Wall built?\", \"answer\": \"Built from 7th century BC\"},\n",
    "        {\"question\": \"Who joined the walls?\", \"answer\": \"Qin Shi Huang\"}\n",
    "    ]}\n",
    "    '''\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = f\"\"\"From this Wikipedia content, create 8 high-quality question-answer pairs.\n",
    "\n",
    "Content: {wiki_content}\n",
    "\n",
    "Each pair needs a specific question and complete answer from the text.\n",
    "You MUST respond with ONLY valid JSON in this exact format: {json_format}\n",
    "Do not include any explanatory text, only the JSON.\"\"\"\n",
    "    \n",
    "    # Get response from the LLM (WITHOUT format='json' to avoid npcpy bug)\n",
    "    response = data_creator.get_llm_response(prompt)\n",
    "    response_text = response['response']\n",
    "    \n",
    "    # Extract JSON from response (handles cases where LLM adds extra text)\n",
    "    try:\n",
    "        # Try to find JSON in the response\n",
    "        json_match = re.search(r'\\{.*\"pairs\".*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(0)\n",
    "            qa_data = json.loads(json_str)\n",
    "            qa_pairs = qa_data['pairs']\n",
    "        else:\n",
    "            # If no JSON found, try parsing the whole response\n",
    "            qa_data = json.loads(response_text)\n",
    "            qa_pairs = qa_data['pairs']\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        print(f\"Raw response: {response_text}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    df = pd.DataFrame(qa_pairs)\n",
    "    df.to_csv('wikipedia_qa_dataset.csv', index=False)\n",
    "    \n",
    "    print(f\"Created {len(qa_pairs)} Q&A pairs, saved to wikipedia_qa_dataset.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run the Demo\n",
    "\n",
    "Let's generate a Q&A dataset about the Great Wall of China!\n",
    "\n",
    "You can change the topic to anything you're interested in:\n",
    "- `create_qa_dataset_demo(\"Artificial Intelligence\")`\n",
    "- `create_qa_dataset_demo(\"Quantum Computing\")`\n",
    "- `create_qa_dataset_demo(\"Machine Learning\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "qa_dataset = create_qa_dataset_demo()\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nGenerated Q&A Pairs:\")\n",
    "print(\"=\" * 80)\n",
    "qa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have a Q&A dataset, you can:\n",
    "\n",
    "1. **Generate more datasets**: Run this for multiple topics\n",
    "2. **Combine datasets**: Merge multiple CSV files for a larger training set\n",
    "3. **Fine-tune a model**: Use this data for supervised fine-tuning\n",
    "4. **Evaluate quality**: Review the Q&A pairs and filter low-quality ones\n",
    "\n",
    "### Try It Yourself\n",
    "\n",
    "```python\n",
    "# Generate datasets for multiple topics\n",
    "topics = [\"Python Programming\", \"Deep Learning\", \"Natural Language Processing\"]\n",
    "\n",
    "all_datasets = []\n",
    "for topic in topics:\n",
    "    df = create_qa_dataset_demo(topic)\n",
    "    all_datasets.append(df)\n",
    "\n",
    "# Combine all datasets\n",
    "combined_df = pd.concat(all_datasets, ignore_index=True)\n",
    "combined_df.to_csv('combined_qa_dataset.csv', index=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
